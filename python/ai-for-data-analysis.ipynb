{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ollama\n",
    "! pip install pandas\n",
    "! pip install matplotlib\n",
    "! ollama pull gemma3:4b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:4b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'In which year the programming language Java was introduced?',\n",
    "  }\n",
    "],\n",
    "  options={\"temperature\":0.7}\n",
    ")\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccf29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-llms-ollama llama-index-embeddings-ollama pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9270d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# --- 1) Locate the source code ---\n",
    "resources_dir = Path(\"../src/main/java/org/example/\")\n",
    "java_paths = sorted(glob.glob(str(resources_dir / \"*.java\")))\n",
    "if not java_paths:\n",
    "    raise FileNotFoundError(\"No Java files found in ./input. Please place one there.\")\n",
    "\n",
    "# --- 2) Configure Ollama LLM and embedding model ---\n",
    "Settings.llm = Ollama(model=\"gemma3:4b\", request_timeout=120.0)\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"embeddinggemma\")\n",
    "\n",
    "# --- 3) Load and index the Java code with embeddings ---\n",
    "docs = SimpleDirectoryReader(input_files=[pdf_paths[0]]).load_data()\n",
    "index = VectorStoreIndex.from_documents(docs)\n",
    "\n",
    "# --- 4) Run a query ---\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "prompt = \"Build three test cases to improve the mutation score of Main.java.\"\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "print(f\"Queried file: {Path(pdf_paths[0]).name}\")\n",
    "print(\"\\n=== Response ===\\n\")\n",
    "print(str(response))\n",
    "\n",
    "# --- 5) Append the test cases to the original file ---\n",
    "\n",
    "# Path to your test file\n",
    "test_file_path = Path(\"../src/test/java/org/example/MainTest.java\")\n",
    "\n",
    "# Read the existing test file\n",
    "with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_content = f.read()\n",
    "\n",
    "# Extract model output as text\n",
    "new_tests = str(response).strip()\n",
    "\n",
    "# Basic sanity: ensure the model didn’t include extra import/class definitions\n",
    "# If it only includes methods, we’ll inject them before the final closing brace\n",
    "if \"class \" not in new_tests and \"}\" in test_content:\n",
    "    # Insert the test cases before the last closing brace\n",
    "    test_content = test_content.rstrip()\n",
    "    test_content = test_content[:-1] + \"\\n\\n    \" + new_tests.replace(\"\\n\", \"\\n    \") + \"\\n}\"\n",
    "else:\n",
    "    # If the response includes a full class definition, just append it at the end\n",
    "    test_content += \"\\n\\n\" + new_tests\n",
    "\n",
    "# Write the updated test file back\n",
    "with open(test_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(test_content)\n",
    "\n",
    "print(f\"✅ Appended new test cases to: {test_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d21e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b27818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe an image with Gemma3:4b via Ollama\n",
    "\n",
    "\n",
    "import ollama\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1) Path to your image ---\n",
    "image_path = \"./input/rag_simple.png\"  # <-- replace with your image\n",
    "\n",
    "# Optional: preview image in the notebook\n",
    "display(Image.open(image_path))\n",
    "\n",
    "# --- 2) Send prompt + image to Ollama ---\n",
    "response = ollama.chat(\n",
    "    model=\"gemma3:4b\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Given the image, build me an Agent AI workflow in Python using Ollama for testing Java software, including code coverage and mutation testing.\",\n",
    "            \"images\": [image_path],  # send the image to the model\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# --- 3) Print the description ---\n",
    "print(\"=== Image Description ===\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
